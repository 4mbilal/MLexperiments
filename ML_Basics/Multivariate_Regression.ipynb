{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multivariate Regression for House Pricing Dataset**\n",
    "\n",
    "1- Try different learning rates  \n",
    "2- Try with/without feature scaling  \n",
    "3- Try polynomial regression  \n",
    "4- Try Scikit library  \n",
    "5- It overfits even for linear case. Notice extreme overfitting when polynomial degree is 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_csv(filename):\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        data_reader = csv.reader(csvfile)\n",
    "        data = list(data_reader)[1:]  # Ignore the first row\n",
    "        data = [row[1:] for row in data]  # Ignore the first column\n",
    "    return data\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    # Transpose the data to work with columns\n",
    "    data_transposed = list(zip(*data))\n",
    "    \n",
    "    # Replace 'NA' with the mean of the column\n",
    "    for i in range(len(data_transposed)):\n",
    "        col = [float(x) if x != 'NA' else None for x in data_transposed[i]]\n",
    "        mean_val = sum(x for x in col if x is not None) / len([x for x in col if x is not None])\n",
    "        col = [x if x is not None else mean_val for x in col]\n",
    "        data_transposed[i] = col\n",
    "    \n",
    "    # Transpose back to original format\n",
    "    data = list(zip(*data_transposed))\n",
    "    return data\n",
    "\n",
    "def feature_scaling(features):\n",
    "    scaled_features = []\n",
    "    for i in range(len(features[0])):\n",
    "        col = [row[i] for row in features]\n",
    "        min_val = min(col)\n",
    "        max_val = max(col)\n",
    "        scaled_features.append([(x - min_val) / (max_val - min_val) for x in col])\n",
    "    scaled_features = list(zip(*scaled_features))\n",
    "    return scaled_features\n",
    "\n",
    "def add_polynomial_features(features, degree=2):\n",
    "    poly_features = []\n",
    "    for row in features:\n",
    "        poly_row = []\n",
    "        for val in row:\n",
    "            for d in range(1, degree+1):\n",
    "                poly_row.append(val ** d)\n",
    "        poly_features.append(poly_row)\n",
    "    return poly_features\n",
    "\n",
    "def split_features_labels(data):\n",
    "    features = [list(map(float, row[:-1])) for row in data]\n",
    "    labels = [float(row[-1]) for row in data]\n",
    "    return features, labels\n",
    "\n",
    "def predict(features, weights):\n",
    "    return [sum(w * x for w, x in zip(weights, row)) for row in features]\n",
    "\n",
    "def compute_cost(features, labels, weights):\n",
    "    m = len(labels)\n",
    "    predictions = predict(features, weights)\n",
    "    return sum((pred - true) ** 2 for pred, true in zip(predictions, labels)) / (2 * m)\n",
    "\n",
    "def gradient_descent(train_features, train_labels, test_features, test_labels, weights, learning_rate, epochs):\n",
    "    m_train = len(train_labels)\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_predictions = predict(train_features, weights)\n",
    "        test_predictions = predict(test_features, weights)\n",
    "        \n",
    "        gradients = [0] * len(weights)\n",
    "        for i in range(len(weights)):\n",
    "            gradients[i] = sum((pred - true) * train_features[j][i] for j, (pred, true) in enumerate(zip(train_predictions, train_labels))) / m_train\n",
    "        weights = [w - learning_rate * g for w, g in zip(weights, gradients)]\n",
    "        \n",
    "        train_loss = compute_cost(train_features, train_labels, weights)\n",
    "        test_loss = compute_cost(test_features, test_labels, weights)\n",
    "        \n",
    "        train_loss_history.append(train_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "        \n",
    "        # Optional: Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "    \n",
    "    return weights, train_loss_history, test_loss_history\n",
    "\n",
    "# Load data\n",
    "train_data = load_csv('house_price_train_subset.csv')\n",
    "test_data = load_csv('house_price_test_subset.csv')\n",
    "\n",
    "# Handle missing values\n",
    "train_data = handle_missing_values(train_data)\n",
    "test_data = handle_missing_values(test_data)\n",
    "\n",
    "# Split features and labels\n",
    "train_features, train_labels = split_features_labels(train_data)\n",
    "test_features, test_labels = split_features_labels(test_data)\n",
    "\n",
    "# Feature scaling\n",
    "train_features = feature_scaling(train_features)\n",
    "test_features = feature_scaling(test_features)\n",
    "\n",
    "# Add polynomial features\n",
    "polynomial_degree = 2\n",
    "train_features = add_polynomial_features(train_features, degree=polynomial_degree)\n",
    "test_features = add_polynomial_features(test_features, degree=polynomial_degree)\n",
    "\n",
    "# Initialize weights\n",
    "weights = [0.0] * len(train_features[0])\n",
    "\n",
    "# Train model\n",
    "learning_rate = 0.1\n",
    "epochs = 400\n",
    "weights, train_loss_history, test_loss_history = gradient_descent(train_features, train_labels, test_features, test_labels, weights, learning_rate, epochs)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), train_loss_history, label='Train Loss')\n",
    "plt.plot(range(epochs), test_loss_history, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curves')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('house_price_train_subset.csv', index_col=0)\n",
    "test_data = pd.read_csv('house_price_test_subset.csv', index_col=0)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_data_imputed = imputer.fit_transform(train_data)\n",
    "test_data_imputed = imputer.transform(test_data)\n",
    "\n",
    "# Split features and labels\n",
    "train_features = train_data_imputed[:, :-1]\n",
    "train_labels = train_data_imputed[:, -1]\n",
    "test_features = test_data_imputed[:, :-1]\n",
    "test_labels = test_data_imputed[:, -1]\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Add polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "train_features_poly = poly.fit_transform(train_features_scaled)\n",
    "test_features_poly = poly.transform(test_features_scaled)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(train_features_poly, train_labels)\n",
    "\n",
    "# Predictions\n",
    "train_predictions = model.predict(train_features_poly)\n",
    "test_predictions = model.predict(test_features_poly)\n",
    "\n",
    "# Calculate loss\n",
    "train_loss = mean_squared_error(train_labels, train_predictions) / 2\n",
    "test_loss = mean_squared_error(test_labels, test_predictions) / 2\n",
    "\n",
    "print(f'Train Loss: {train_loss}')\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
