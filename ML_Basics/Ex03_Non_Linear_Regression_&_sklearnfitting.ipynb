{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try overfitting a high-order polynomial on the given points.  \n",
    "The 'fit_polynomial' function is generating the Least Square Matrix.  \n",
    "An n-degree polynomial can fit n+1 points (provided x values are unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_points(m, c, num_points, noise_std=2):\n",
    "    \"\"\"Generates random points along a line with Gaussian noise.\"\"\"\n",
    "    x = np.random.uniform(-10, 10, num_points)\n",
    "    noise = np.random.normal(0, noise_std, x.shape)\n",
    "    y = m * x + c + noise\n",
    "    return x, y\n",
    "\n",
    "def fit_polynomial(x, y, degree):\n",
    "    \"\"\"Fits a polynomial of given degree to the points (x, y) using least squares.\"\"\"\n",
    "    n = len(x)\n",
    "    X = np.vstack([x**i for i in range(degree + 1)]).T\n",
    "    \n",
    "    # Construct the matrix for least squares fitting\n",
    "    A = np.zeros((degree + 1, degree + 1))\n",
    "    b = np.zeros(degree + 1)\n",
    "    \n",
    "    for i in range(degree + 1):\n",
    "        for j in range(degree + 1):\n",
    "            A[i, j] = np.sum(x**(i + j))\n",
    "        b[i] = np.sum(y * x**i)\n",
    "    \n",
    "    # Perform Gauss-Jordan elimination to find the inverse of A\n",
    "    def gauss_jordan(A):\n",
    "        n = len(A)\n",
    "        augmented_matrix = np.hstack((A, np.eye(n)))\n",
    "        for i in range(n):\n",
    "            factor = augmented_matrix[i, i]\n",
    "            augmented_matrix[i] = augmented_matrix[i] / factor\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    factor = augmented_matrix[j, i]\n",
    "                    augmented_matrix[j] -= factor * augmented_matrix[i]\n",
    "        return augmented_matrix[:, n:]\n",
    "\n",
    "    inv_A = gauss_jordan(A)\n",
    "    coefficients = np.dot(inv_A, b)\n",
    "    return coefficients\n",
    "\n",
    "def plot_points_and_curve(x, y, coefficients):\n",
    "    \"\"\"Plots the random points and the fitted polynomial curve.\"\"\"\n",
    "    x_line = np.linspace(-10, 10, 100)\n",
    "    y_line = sum(coefficients[i] * x_line**i for i in range(len(coefficients)))\n",
    "    \n",
    "    plt.scatter(x, y, color='red', label='Random Points')\n",
    "    plt.plot(x_line, y_line, color='blue', label='Fitted Polynomial')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Random Points and Fitted Polynomial Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xlim(-30, 30)  # Set x-axis limits\n",
    "    plt.ylim(-30, 30)  # Set y-axis limits\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "m = 2\n",
    "c = 5\n",
    "num_points = 5\n",
    "degree = 4\n",
    "\n",
    "x, y = generate_points(m, c, num_points)\n",
    "coefficients = fit_polynomial(x, y, degree)\n",
    "plot_points_and_curve(x, y, coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a basic Gradient Descent Algorithm for comparison with analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_points(m, c, num_points, noise_std=2):\n",
    "    \"\"\"Generates random points along a line with Gaussian noise.\"\"\"\n",
    "    x = np.random.uniform(-10, 10, num_points)\n",
    "    noise = np.random.normal(0, noise_std, x.shape)\n",
    "    y = m * x + c + noise\n",
    "    return x, y\n",
    "\n",
    "def fit_polynomial(x, y, degree):\n",
    "    \"\"\"Fits a polynomial of given degree to the points (x, y) using least squares.\"\"\"\n",
    "    n = len(x)\n",
    "    X = np.vstack([x**i for i in range(degree + 1)]).T\n",
    "    \n",
    "    # Construct the matrix for least squares fitting\n",
    "    A = np.zeros((degree + 1, degree + 1))\n",
    "    b = np.zeros(degree + 1)\n",
    "    \n",
    "    for i in range(degree + 1):\n",
    "        for j in range(degree + 1):\n",
    "            A[i, j] = np.sum(x**(i + j))\n",
    "        b[i] = np.sum(y * x**i)\n",
    "    \n",
    "    # Perform Gauss-Jordan elimination to find the inverse of A\n",
    "    def gauss_jordan(A):\n",
    "        n = len(A)\n",
    "        augmented_matrix = np.hstack((A, np.eye(n)))\n",
    "        for i in range(n):\n",
    "            factor = augmented_matrix[i, i]\n",
    "            augmented_matrix[i] = augmented_matrix[i] / factor\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    factor = augmented_matrix[j, i]\n",
    "                    augmented_matrix[j] -= factor * augmented_matrix[i]\n",
    "        return augmented_matrix[:, n:]\n",
    "\n",
    "    inv_A = gauss_jordan(A)\n",
    "    coefficients = np.dot(inv_A, b)\n",
    "    return coefficients\n",
    "\n",
    "def gradient_descent(x, y, learning_rate=0.01, iterations=1000, degree=2):\n",
    "    x = x/10\n",
    "    # y = y/10\n",
    "    # degree = 2\n",
    "    \"\"\"Performs gradient descent to fit a linear regression model.\"\"\"\n",
    "    n = len(x)\n",
    "    theta = np.zeros(degree + 1)  # Initialize coefficients for polynomial of given degree\n",
    "    # X = np.vstack((np.ones(n), x)).T\n",
    "    X = np.vstack([x**i for i in range(degree + 1)]).T\n",
    "\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        predictions = np.dot(X, theta).reshape(-1, 1)\n",
    "        # print(theta)\n",
    "        errors = predictions - y\n",
    "        gradient = (2 / n) * np.dot(X.T, errors)\n",
    "        theta -= learning_rate * gradient.flatten()\n",
    "        \n",
    "        # Compute and store the loss\n",
    "        loss = (1 / n) * np.sum(errors ** 2)\n",
    "        loss_history.append(loss)\n",
    "    \n",
    "    return theta, loss_history\n",
    "\n",
    "def plot_points_and_curves(x, y, normal_eq_coeff, gd_coeff):\n",
    "    \"\"\"Plots the random points and the fitted linear curves in separate figures.\"\"\"\n",
    "    x_line = np.linspace(-10, 10, 100)\n",
    "    degree = len(gd_coeff) - 1\n",
    "    y_gd = 0\n",
    "    for i in range(len(gd_coeff)):\n",
    "        y_gd += gd_coeff[i] * (x_line/10)**i\n",
    "\n",
    "    y_normal_eq = 0\n",
    "    for i in range(len(normal_eq_coeff)):\n",
    "        y_normal_eq += normal_eq_coeff[i] * x_line**i\n",
    "\n",
    "\n",
    "    \n",
    "    # Plot the normal equation solution\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y, color='red', label='Random Points')\n",
    "    plt.plot(x_line, y_normal_eq, color='blue', label='Normal Equation')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Linear Regression: Normal Equation Solution')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-30, 30)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the gradient descent solution\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y, color='red', label='Random Points')\n",
    "    plt.plot(x_line, y_gd, color='green', label='Gradient Descent')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Linear Regression: Gradient Descent Solution')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-30, 30)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_curve(loss_history):\n",
    "    \"\"\"Plots the loss curve for gradient descent.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(loss_history, label='Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Gradient Descent Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "m = 2\n",
    "c = 5\n",
    "num_points = 15\n",
    "degree = 2\n",
    "\n",
    "x, y = generate_points(m, c, num_points)\n",
    "print(x)\n",
    "normal_eq_coeff = fit_polynomial(x, y, degree)\n",
    "\n",
    "learning_rate=1e-1\n",
    "iterations=int(1.0e4)\n",
    "\n",
    "gd_coeff, loss_history = gradient_descent(x, y, learning_rate, iterations, degree)\n",
    "\n",
    "print(\"Normal Equation Coefficients:\", normal_eq_coeff)\n",
    "print(\"Gradient Descent Coefficients:\", gd_coeff)\n",
    "\n",
    "plot_loss_curve(loss_history)\n",
    "plot_points_and_curves(x, y, normal_eq_coeff, gd_coeff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curve Fitting using sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Simulate noisy linear data\n",
    "num_points = 10\n",
    "noise_std = 2.0\n",
    "m = 1.5\n",
    "c = -3.0\n",
    "\n",
    "x = np.random.uniform(-10, 10, num_points)\n",
    "noise = np.random.normal(0, noise_std, x.shape)\n",
    "y = m * x + c + noise\n",
    "\n",
    "# Reshape x for sklearn\n",
    "x = x.reshape(-1, 1)\n",
    "\n",
    "# Polynomial degree\n",
    "degree = 9  # You can increase this for more complex curves\n",
    "\n",
    "# Fit polynomial model\n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(x)\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Generate smooth x range for plotting\n",
    "x_line = np.linspace(-10, 10, 100).reshape(-1, 1)\n",
    "X_line_poly = poly.transform(x_line)\n",
    "y_line_fit = model.predict(X_line_poly)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, color='blue', label='Noisy data', alpha=0.6)\n",
    "plt.plot(x_line, y_line_fit, color='red', linewidth=2, label=f'Polynomial fit (degree {degree})')\n",
    "plt.title('Polynomial Curve Fitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
